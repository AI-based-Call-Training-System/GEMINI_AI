# chunk-audio-api/gemini/gemini_module.py
import os
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
# LLMChain ëŒ€ì‹  LCELì„ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.
# from langchain.chains import LLMChain 
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.output_parser import StrOutputParser
from operator import itemgetter # LCELì—ì„œ ì…ë ¥ì„ êµ¬ì¡°í™”í•˜ê¸° ìœ„í•´ ì‚¬ìš©

from gemini.prompt_module import choose_chat_prompt
from db.history_module import get_user_history

# 1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2ï¸âƒ£ Gemini ëª¨ë¸ ì´ˆê¸°í™” (LangChainìš©)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# 3ï¸âƒ£ ì‚¬ìš©ìë³„ ì„¸ì…˜ ì²˜ë¦¬ í•¨ìˆ˜
def ask_gemini(session_id: str, user_input: str, scenario: str) -> str:
    try:
        # DBì—ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (session_id ë˜ëŠ” user_id ê¸°ë°˜)
        history_data = get_user_history(session_id) 

        # LangChain Memory ê°ì²´ ìƒì„± ë° ê³¼ê±° íˆìŠ¤í† ë¦¬ ë¡œë“œ
        memory = ConversationBufferMemory(
            memory_key="history",
            input_key="user_input",
            return_messages=True,
        )
        
        for turn in history_data:
            if turn["role"] == "user":
                memory.chat_memory.add_user_message(turn["content"])
            elif turn["role"] == "gemini":
                memory.chat_memory.add_ai_message(turn["content"])

        # ğŸ’¡ LCEL (LangChain Expression Language)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±
        
        # 4ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (MessagesPlaceholderë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì£¼ì…)
        # choose_chat_prompt(scenario)ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        
        # history: ê³¼ê±° ëŒ€í™” ë‚´ìš©ì´ ë“¤ì–´ê°ˆ ìë¦¬
        # user_input: í˜„ì¬ ì‚¬ìš©ìì˜ ì…ë ¥ì´ ë“¤ì–´ê°ˆ ìë¦¬
        prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system", choose_chat_prompt(scenario)),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{user_input}")
            ]
        )

        # 5ï¸âƒ£ ìµœì¢… ì²´ì¸ êµ¬ì„± (Runnable Sequence)
        # itemgetter("user_input")ìœ¼ë¡œ í˜„ì¬ ì…ë ¥ì„ ì „ë‹¬í•˜ê³ ,
        # itemgetter("history")ë¡œ ë©”ëª¨ë¦¬(ê³¼ê±° ëŒ€í™”)ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
        
        # {
        #    "history": memory.load_memory_variables({})["history"], # ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë¡œë“œ
        #    "user_input": itemgetter("user_input") # í˜„ì¬ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        # }
        
        chain = (
            {
                "history": itemgetter("history_messages"), # ë©”ëª¨ë¦¬ ê°ì²´ì—ì„œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                "user_input": itemgetter("user_input"),
            }
            | prompt_template 
            | llm
            | StrOutputParser() # ì‘ë‹µì„ ê¹”ë”í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
        )

        # 6ï¸âƒ£ ë©”ëª¨ë¦¬ ë° ì…ë ¥ ë°ì´í„°ë¥¼ ì¤€ë¹„
        # invokeì— ì „ë‹¬í•  ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“­ë‹ˆë‹¤.
        # ë©”ëª¨ë¦¬ ë¡œë“œ: memory.load_memory_variables({})ëŠ” {"history": [Message, Message, ...]} í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
        # ë”°ë¼ì„œ "history_messages" ëŒ€ì‹  ë©”ëª¨ë¦¬ í‚¤ì¸ "history"ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        
        input_data = {
            # memory.load_memory_variables({})ì—ì„œ memory_keyì¸ "history"ì˜ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            "history_messages": memory.load_memory_variables({})["history"], 
            "user_input": user_input
        }
        
        # 7ï¸âƒ£ ëª¨ë¸ í˜¸ì¶œ (invoke ì‚¬ìš©)
        response = chain.invoke(input_data)
        
        # ì‘ë‹µ í›„ í˜„ì¬ ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•´)
        memory.save_context({"user_input": user_input}, {"output": response})
        
        return response.strip()

    except Exception as e:
        return f"[Gemini ì˜¤ë¥˜] {str(e)}"