# chunk-audio-api/gemini/gemini_module.py

import os
from dotenv import load_dotenv
from operator import itemgetter 

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.output_parser import StrOutputParser
from db.history_module import get_user_history
# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from gemini.prompt_module import choose_chat_prompt
# from db.history_module import get_user_history # ì‹¤ì œ DB ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ ì„ì‹œ í•¨ìˆ˜ë¡œ ëŒ€ì²´

# ----------------------------------------------------
# ì„ì‹œ DB í•¨ìˆ˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” DB ëª¨ë“ˆë¡œ ëŒ€ì²´ë˜ì–´ì•¼ í•©ë‹ˆë‹¤)


# 1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2ï¸âƒ£ Gemini ëª¨ë¸ ì´ˆê¸°í™” (LangChainìš©)
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: GEMINI_API_KEY=YOUR_API_KEY
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# 3ï¸âƒ£ ì‚¬ìš©ìë³„ ì„¸ì…˜ ì²˜ë¦¬ í•¨ìˆ˜
def ask_gemini(session_id: str, user_input: str, scenario: str) -> str:
    try:
        # DBì—ì„œ ê³¼ê±° íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (session_id ë˜ëŠ” user_id ê¸°ë°˜)
        history_data = get_user_history(session_id) 

        # LangChain Memory ê°ì²´ ìƒì„± ë° ê³¼ê±° íˆìŠ¤í† ë¦¬ ë¡œë“œ
        memory = ConversationBufferMemory(
            memory_key="history",
            input_key="user_input",
            return_messages=True,
        )
        
        for turn in history_data:
            if turn["role"] == "user":
                memory.chat_memory.add_user_message(turn["content"])
            elif turn["role"] == "gemini":
                memory.chat_memory.add_ai_message(turn["content"])

        # 4ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ 
        system_message = choose_chat_prompt(scenario)
        
        prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system", system_message),
                MessagesPlaceholder(variable_name="history"), # ë³€ìˆ˜: history
                ("human", "{user_input}")                    # ë³€ìˆ˜: user_input
            ]
        )

        # 5ï¸âƒ£ ìµœì¢… ì²´ì¸ êµ¬ì„± (Runnable Sequence)
        # ğŸš¨ ìˆ˜ì •: itemgetter í‚¤ë¥¼ "history"ì™€ "user_input"ìœ¼ë¡œ í†µì¼
        chain = (
            {
                "history": itemgetter("history"),  
                "user_input": itemgetter("user_input"),
            }
            | prompt_template 
            | llm
            | StrOutputParser()
        )

        # 6ï¸âƒ£ ë©”ëª¨ë¦¬ ë° ì…ë ¥ ë°ì´í„°ë¥¼ ì¤€ë¹„
        # ğŸš¨ ìˆ˜ì •: input_dataì˜ í‚¤ë¥¼ "history"ë¡œ í†µì¼
        input_data = {
            # memory.load_memory_variables({})ì—ì„œ memory_keyì¸ "history"ì˜ ê°’ì„ ê°€ì ¸ì˜´
            "history": memory.load_memory_variables({})["history"], 
            "user_input": user_input
        }
        
        # 7ï¸âƒ£ ëª¨ë¸ í˜¸ì¶œ (invoke ì‚¬ìš©)
        response = chain.invoke(input_data)
        
        # ì‘ë‹µ í›„ í˜„ì¬ ëŒ€í™”ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•´)
        memory.save_context({"user_input": user_input}, {"output": response})
        
        return response.strip()

    except Exception as e:
        return f"[Gemini ì˜¤ë¥˜] {str(e)}"
